<context>
# Overview
This project, the "Personal Networking Hub Web Application," aims to build a software tool that helps a user manage and enrich their professional networking efforts. For the authenticated user, it will strive to access their LinkedIn profile data (education, experience, skills) via official LinkedIn APIs after an OAuth 2.0 login. It may also automate in-depth research into alumni from specified educational institutions (Questrom, University School) and product managers based in New York City for broader discovery, and provide a dedicated web interface for managing these leads and insights. It solves the problem of highly time-consuming manual research and data consolidation. This tool is for a proactive job seeker or professional, likely with a product management focus and technical aptitude, who wants to leverage their own LinkedIn data and efficiently explore their network and target opportunities. It's valuable because it aims to provide curated and contextualized leads and insights, save significant time, and allow for a more strategic approach to networking.

# Core Features
- User Authentication & LinkedIn Profile Data Access
  - What it does: Allows the user to authenticate with their LinkedIn account via OAuth 2.0. Once authenticated, the application will attempt to access the user's own detailed profile information (name, profile URL, current role, company, location, education, experience, skills) using official LinkedIn APIs.
  - Why it's important: Provides secure access to the user's foundational professional data, which can then be used by the application to offer personalized insights and assistance. This is the most robust and ToS-compliant method for accessing user-specific data.
  - How it works at a high level: Implements the LinkedIn OAuth 2.0 authorization flow. Uses the obtained access token to make requests to relevant LinkedIn Profile APIs.

- User Networking Goals & Preferences
  - What it does: Allows the user to define, store, and manage their specific networking and job search preferences (e.g., target roles, industries, company stages, desired locations, key skills to look for or match against).
  - Why it's important: Enables the application to personalize discovery, tailor contextual insights, and refine lead generation based on the user's explicit goals, making the assistant more effective.
  - How it works at a high level: User inputs preferences via the web application. These are stored in the internal database and used by various modules (data acquisition, curation, insight generation) to filter and prioritize information.

- LinkedIn Data Acquisition (Broader Discovery - Alumni & NYC PMs)
  - What it does: Optionally, automatically searches and extracts public data from LinkedIn profiles of alumni from Questrom and University School, as well as Product Managers working in New York City. Key data includes name, profile URL, current role, company, and location. This is for general discovery, separate from the authenticated user's direct profile data.
  - Why it's important: This can supplement the user's network with new potential contacts based on broader criteria.
  - How it works at a high level: Utilizes web scraping techniques, potentially through a browser automation tool like Selenium (used cautiously and with robust error handling) or by leveraging third-party scraping APIs (e.g., SerpApi), to parse public LinkedIn search result pages. This method is subject to LinkedIn's ToS and anti-scraping measures.

- Internal Lead Database
  - What it does: Stores all processed data, including the authenticated user's profile information (if fetched), discovered leads, and associated contextual information, in a local, internal database (e.g., SQLite). This database will serve as the single source of truth for the web application.
  - Why it's important: Provides persistent, structured storage, enabling efficient retrieval, filtering, and management.
  - How it works at a high level: A database schema will be defined. Python scripts will interact with the database (e.g., using SQLAlchemy or direct SQLite commands) to insert, update, and query data.

- Web Application (Backend & Frontend)
  - What it does: Provides a user interface for the user to view their profile insights, manage discovered leads, and interact with other application features.
  - Why it's important: This is the primary way the user will interact with and benefit from the application.
  - How it works at a high level:
    - Backend: A Python web framework (e.g., Flask, FastAPI) will expose a comprehensive set of API endpoints designed to serve the initial web application and be extensible for future clients, including a more advanced web dashboard and a browser extension. These endpoints will handle OAuth, interact with LinkedIn APIs, query the internal database, and return data.
    - Frontend: Built using Streamlit (for rapid development) or a custom HTML/CSS/JavaScript stack. It will consume data from the backend API and present it in tables, allow filtering, and provide lead management functionalities such as status tracking (e.g., contacted, replied), adding notes, and prioritization/tagging.

- Company Page Information Retrieval (for Discovered Leads)
  - What it does: Optionally fetches basic public information about a *discovered lead's* current company (not necessarily the authenticated user's company, unless they are also a lead).
  - Why it's important: Provides valuable contextual insights about the companies where discovered leads are employed.
  - How it works at a high level: Involves a secondary scraping operation targeting LinkedIn company pages or other public business directories.

- Job Board Integration (Lever & Greenhouse)
  - What it does: Cross-references companies (of user or discovered leads) with open product management roles in New York City by integrating with Lever and Greenhouse job board APIs.
  - Why it's important: Directly connects contacts with potential job opportunities.
  - How it works at a high level: Makes API calls to Lever and Greenhouse.

- Lead Curation & Filtering Logic (for Discovered Leads)
  - What it does: Applies predefined filters to raw *discovered lead* data.
  - Why it's important: Ensures the user receives a manageable and relevant list of *discovered* leads.
  - How it works at a high level: Programmatically evaluates each collected *discovered* lead.

- Contextual Insight Generation
  - What it does: Enriches the authenticated user's profile view and *discovered leads* with additional contextual information.
  - Why it's important: Provides a richer understanding for more effective networking.
  - How it works at a high level: Consolidates information from various sources.

- Data Export & Presentation
  - (Content largely unchanged, focuses on web app primarily)

# User Experience
- User Personas:
  - The primary user is a proactive individual seeking to leverage their own LinkedIn profile, define specific networking goals, and network effectively, and discover new relevant contacts based on those goals.
- Key User Flows:
  - Authentication: User securely logs in with their LinkedIn account via OAuth 2.0.
  - Profile View & Insights: Application displays the user's own LinkedIn profile data fetched via API, potentially with derived insights.
  - Automated Discovery (Optional): User initiates a discovery run for alumni or NYC PMs.
    - (Process similar to before, but clearly distinct from user's own data access)
  - Lead Review & Management (Web Application):
    - User reviews discovered leads and their own contextualized data, with capabilities to track status (e.g., contacted, replied, meeting scheduled), add notes, and prioritize or tag leads within the web application.
  - Action & Outreach: User utilizes insights from their own profile and discovered leads.
- UI/UX Considerations (Web Application):
  - (Content largely unchanged)

# Technical Architecture
- System Components:
  - User Authentication Module (Python, Backend):
    - Handles LinkedIn OAuth 2.0 flow.
    - Securely manages access tokens.
  - LinkedIn API Client Module (Python):
    - Interacts with official LinkedIn APIs using authenticated user's token to fetch profile data (own profile, possibly connections if API permits and scope allows).
  - Data Acquisition Module (Python - for Broader Discovery):
    - LinkedIn Scraper (for discovery): Utilizes Selenium (cautiously) or third-party APIs (SerpApi) for *public, unauthenticated searches* for alumni/NYC PMs. BeautifulSoup for parsing if needed.
    - Company Info Scraper (for discovery): Similar technology for company pages related to *discovered leads*.
  - API Integration Module (Python - for Job Boards):
    - (Content largely unchanged: Lever & Greenhouse clients)
  - Data Processing & Curation Module (Python):
    - (Content largely unchanged, clarifies application to discovered leads primarily)
  - Internal Database (e.g., SQLite):
    - (Content largely unchanged, schema may adapt slightly for user's own profile data vs. leads)
  - Web Application Backend (Python - e.g., Flask, FastAPI):
    - (Content largely unchanged, adds OAuth handling)
  - Web Application Frontend (e.g., Streamlit, or HTML/CSS/JS):
    - (Content largely unchanged)
  - Configuration Manager (Python):
    - (Content largely unchanged, will include LinkedIn App Client ID/Secret)

- Data Models (for internal database):
  - UserProfile Table (for authenticated user's data from API):
    - user_id: String (LinkedIn User ID, Primary Key)
    - full_name: String
    - profile_url: String
    - headline: String
    - location: String
    - summary: Text
    - (Structured fields for experience, education, skills, etc., possibly as JSON or separate related tables)
    - last_api_sync: Datetime
  - UserPreferences Table:
    - user_id: String (Foreign Key to UserProfile, Primary Key)
    - target_roles: JSON or Text (List of target job titles)
    - target_industries: JSON or Text (List of target industries)
    - target_company_stages: JSON or Text (e.g., startup, Series B, public)
    - target_locations: JSON or Text (List of preferred locations)
    - keywords_for_matching: JSON or Text (Skills or topics to prioritize)
    - last_updated: Datetime
  - Lead Table:
    - (Schema remains similar, but clearly for *discovered* leads. May link to UserProfile if a lead is a connection of the user.)
  - (CompanyTable, JobPostingTable remain similar)

- APIs and Integrations (External):
  - LinkedIn OAuth 2.0: For user authentication.
  - LinkedIn Official APIs (e.g., Profile API, Connections API - specific endpoints TBD by research): For accessing authenticated user's data.
  - (SerpApi or direct scraping for *public discovery only*)
  - Lever API: (Unchanged)
  - Greenhouse Job Board API: (Unchanged)
  - Notion API (Optional): (Unchanged)

- Infrastructure Requirements:
  - (Content largely unchanged)
  - LinkedIn Developer App: User will need to register an application with LinkedIn to get OAuth Client ID/Secret.

# Development Roadmap
- MVP Requirements (Phase 1 - Authentication, User Profile API, Basic Web UI):
  - LinkedIn OAuth 2.0 Integration:
    - Implement "Login with LinkedIn" functionality.
    - Securely handle access tokens.
  - LinkedIn Profile API Integration (User's Own Data):
    - Research and integrate API calls to fetch the authenticated user's detailed profile.
    - Store this data in the UserProfile table.
  - Internal Database Setup (SQLite):
    - Define schema for UserProfile (and Lead table for later).
  - Basic Web Application (Streamlit or Flask/FastAPI + Simple Frontend):
    - Backend: API endpoint to serve authenticated user's profile data.
    - Frontend: Display user's profile data.
  - Basic Configuration:
    - Config for LinkedIn App credentials, database path.

- Future Enhancements (Phase 2 - Broader Discovery, Lead Management, Job Boards):
  - LinkedIn Data Acquisition (Broader Discovery):
    - Implement scraping/SerpApi for alumni and NYC PMs (as a distinct feature). Store in Lead table.
  - Lead Management in Web App:
    - Display discovered leads, filtering, sorting.
  - Job Board Integration:
    - (As before)
  - Contextual Insights - Company Information (for discovered leads):
    - (As before)
  - (Other items from original Phase 2, adapted to new flow)

- Future Enhancements (Phase 3 - Advanced Features & Refinements):
  - (Content largely similar, but features might leverage both user's API-sourced data and discovered leads)
  - Networking Message Assistance:
    - Develop backend capabilities to store message templates and generate personalized message drafts for networking outreach, based on the user's profile, lead data, and defined preferences. This service can be consumed by the web application and future browser extension.
  - Browser Extension Integration:
    - Design and develop a browser extension to provide in-context lead information, facilitate quick actions (e.g., saving a profile to the lead database), and leverage messaging assistance features directly on platforms like LinkedIn.

# Logical Dependency Chain
- Foundation - User Authentication & Core Profile Data:
  - Research and implement LinkedIn OAuth 2.0.
  - Research and integrate LinkedIn Profile API for authenticated user.
- Database Backend:
  - Design and implement UserProfile table (and Lead table structure).
- Core Web Application (Backend & Basic Frontend - MVP):
  - Develop backend API to handle OAuth and serve user's profile data.
  - Develop basic frontend to display user's profile.
- Broader Discovery & Lead Management (Post-MVP for user data):
  - Implement LinkedIn scraping/SerpApi for alumni/NYC PMs (discovery).
  - Enhance web application UI for managing these discovered leads.
- Contextual Enrichment & UI Enhancements:
  - (As before, applied to both user context and discovered leads)

- Pacing and Scoping:
  - (Content largely similar)

# Risks and Mitigations
- Technical Challenges:
  - LinkedIn API Limitations/Access Restrictions: Official APIs might not provide all desired data fields or might have restrictive rate limits.
    - Mitigation: Thorough upfront research of API capabilities. Design application to gracefully handle missing data. Explore alternative official endpoints if available. *As a last resort, for specific data points not available via API for the authenticated user, cautiously consider highly targeted Selenium automation on the user's own authenticated session, with their explicit understanding and consent, and with robust error handling and human-like interaction patterns.*
  - LinkedIn Blocking/Detection (for broader discovery scraping):
    - (Mitigations remain similar: favor APIs where possible even for public data, robust direct scraping techniques, proxies, CAPTCHA handling services if investing in this path).
  - (Other risks like External API Changes, Data Quality, Job Board Tokens remain similar)

# Appendix
- (Content largely unchanged)
